$\huge{Sentiment\ Analysis}$

# 项目说明

## 项目结构

```
Sentiment
├─README.md
├─requirements.txt
├─models
|   ├─CNN.pt
|   ├─DNN.pt
|   └RNN.pt
├─code
|  ├─config.py
|  ├─dataset.py
|  ├─model.py
|  ├─test.py
|  ├─train.py
├─Dataset
|    ├─test.txt
|    ├─train.txt
|    ├─validation.txt
|    └wiki_word2vec_50.bin
```

注：所有操作都应当在项目的根目录下进行。例如测试时，应当以如下的方式进行：

```shell
python code/test.py --m CNN
```

## 参考库

详见`requirements.txt`。

## 使用方式

可供执行的python脚本为`code/train.py`与`code/test.py`。前者的功能是训练模型，后者的功能是测试模型。同时，为了使用不同的模型进行训练/测试，在执行python脚本时可以添加参数。具体来说，脚本的打开方式应当如：

```shell
python code/train.py --m [model]
# or
python code/test.py --m [model]
```

其中`[model]`为可选参数，可以为CNN， RNN或DNN。如果不指定这一项参数，即直接执行python脚本，那么默认将会使用CNN作为模型。

由于训练好的模型过于庞大，**所以还需先进行训练，后测试**。即先执行`train.py`脚本后，再执行`test.py`。这个过程可能会有一些久。

#  模型结构

## CNN

参考了参考文献中CNN的实现方式，即下图：

![image-20230507151143370](https://p.ipic.vip/k9aly4.png)

对于我们使用的词向量模型，模型的流程可以具体化如下：

![未命名绘图](https://p.ipic.vip/3jzy1k.png)

即包含了如下几个步骤：

- 嵌入层：将输入转换为尺寸为`(batch, 1, 50, 50)`的由词向量组成的Tensor。
- 卷积层与池化层：
  - 每个卷积层的核的尺寸为`(batch, 1, kernel_size, embedding_dim=50)`，于是每次经过卷积层以后得到的是尺寸为`(batch, output_channels=20, sentence_length - kernel_size / 2, 1)`的Tensor。
  - 再将其最大池化，得到`(batch, output_channels=20)`的Tensor。
- 分别将输入通过这样的三个卷积层与池化层，并将其拼接起来（通过`cat`方法）。
- 线性层：得到我们想要的二维分类结果。

## RNN

使用了Bi-LSTM作为RNN的模型实现。模型图如下：

![image-20230507155922341](https://p.ipic.vip/sko7ke.png)

嵌入层：得到`(batch, 1, 50, 50)`的词向量构成的Tensor。

- LSTM层：每个RNN Cell分别在两个方向产生隐藏状态。实际的模型实现中包含两个LSTM层，最后使用第二层在各自的最后一个单元的隐藏状态作为下一层的输入。
- 线性层：得到我们想要的二维分类结果。

## MLP/DNN

![20210424164336294](https://p.ipic.vip/p038pl.png)

模型图即是最普通的MLP模型图。具体来说，这里经过了三个线性层：

- 线性层1:将维数转换为30，
- 线性层2:将维数转换为10，
- 线性层3:将维数转换为2。

最后再经过一个激活函数即可。

# 实验结果

## 测试集上的准确率与F-score

采用了sklearn中的`f1_score`方法计算F-score。

结果：

### CNN

![image-20230507212148191](https://p.ipic.vip/vjdjf4.png)

### RNN

![image-20230507162553187](https://p.ipic.vip/2vs7qg.png)

### MLP/DNN

![image-20230507162640793](https://p.ipic.vip/2kfkhy.png)

将结果绘制成下表：

|      | Accuracy  | F-score   |
| ---- | --------- | --------- |
| CNN  | ==0.822== | ==0.822== |
| RNN  | 0.816     | 0.814     |
| DNN  | 0.815     | 0.807     |

可以看出CNN模型的表现最好。

## 在训练集上的训练过程

训练集上的准确率随着训练批次的变化：

![image-20230507185846386](https://p.ipic.vip/67qu5i.png)

验证集上的准确率随着训练批次的变化：

![image-20230507185923767](https://p.ipic.vip/ezdudc.png)

## 模型对比

- 可以看出，三个模型经过五次训练的准确率都达到了95%以上。其中RNN收敛速度最快（同时过拟合地也更快），而CNN的收敛速度最慢（过拟合的程度也最低）。
- 这与模型的特性有关系：RNN（这里使用的是LSTM）可以保留前后词的相对位置关系，在情感分析中，如此的学习方式可能带来更快的学习效率（而CNN主要是共享权值，忽略了数据的序列）。

# 参数选择

## EPOCH

由于过大的`EPOCH`会导致过拟合现象，而过小的`EPOCH`会导致训练结果难以收敛，训练不充分。故决定一个合适的`EPOCH`是相当有必要的。

在实验过程中得出了如下的规律：

- 对于CNN模型，`EPOCH`对于过拟合现象没有大影响。
- 而对于RNN和DNN模型来说，`EPOCH`越大，过拟合现象就越明显。

| RNN   |                    |
| ----- | ------------------ |
| EPOCH | F-score            |
| 5     | 0.814404432132964  |
| 10    | 0.7447447447447447 |

| DNN   |                    |
| ----- | ------------------ |
| EPOCH | F-score            |
| 5     | 0.8067226890756303 |
| 10    | 0.79               |



最终结合测试集上的结果，将CNN, RNN, DNN三个模型的`EPOCH`设定为了`10, 5, 5`。这样总体的训练效率比较可观，在测试集上的准确率也更加高。

## 统一句长

由于数据集的句子长度不一，故这里采取的方式是将句子长度`SentenceLen`统一为`50`。这是因为数据集中的句长，多为$[40, 60]$之间，鲜有长度超过$100$的句子。

在padding过程中，做了如下处理：

- 长度$\geq50$的句子，直接做截断。
- 长度$\lt50$的句子，填充中性词（如`"把"`）

# 问题思考

## 何时停止训练

由前，最终确定的`EPOCH`在不同模型之间有差异，并最终决定为`[10, 5, 5]`。在这次实验中，主要是采取了如下的考虑：

- 观察验证集、测试集上的准确率来决定何时停止。
- 过大的`EPOCH`不可取，因为本次实验的环境为Macbook，没有显卡，训练效率会极慢。

后来，经过思考以后，我觉得可以使用自动化的方式确定`EPOCH`：直到训练集的$Loss$接近收敛即可。若不然，接着训练会导致过拟合现象的发生。

如果验证集具有比较好的“普遍性”，那么通过验证集来调整`EPOCH`应当是较为理想的。不过这次实验的事实表明训练次数对验证集的准确率似乎没什么影响？

## 参数初始化

对于神经网络的不同层，pytorch提供了不同的初始化方式。在这次实验中用到的有：

- Conv2d: Kaiming初始化
- Linear: Kaiming初始化
- LSTM: 均匀分布初始化

本次实验用到的都是pytorch中默认的参数初始化方式。

- 零均值初始化：
  - Xavier初始化：权重被初始化为$[-\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}}]$的均匀分布，适用于激活函数为`tanh`或`sigmoid`的神经网络。
  - He初始化：权重被初始化为$[-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}}]$的均匀分布，适用于激活函数为`ReLU`的神经网络。
- 高斯分布初始化：权重被初始化为服从均值为$0$， 标准差为$\sigma$的高斯分布。适用于大多数场景，但具体需要根据标准差的大小来确定具体适用的场景。
- 正交初始化：将权重初始化为正交矩阵，这保证了参数的初始值具有一定的对称性。故该方法适用于一些需要保持参数对称性的神经网络。

## 过拟合

原因：

- 训练集比较糟糕（例如训练集过小）。
- 模型设置的参数过多。

- 在学习过程中把训练集的噪声也学习进去了。

解决方案：

- 适当选取好的训练集，例如可以增大训练集的容量。
- 选取更加简单的模型。
- 确定合适的停止训练时间。
- 对模型的参数进行不断调整。
- 正则化：通过在 loss 中增加惩罚项，限制了非线性函数中高次项系数的大小，相当于避免了其过于复杂，从而提高泛化能力。

## 模型对比

就模型的结果而言，前文已经提及。

优点：

- CNN：可以并行计算，计算速度较快，而且提取了局部特征。相比于DNN没有庞大的中间层。
- RNN：携带了时序（序列）信息。
- DNN：可以并行计算，而且编程实现与理解都较为容易。

缺点：

- CNN：在这次实验中由于对句子进行了截取，故可能丢失信息。此外，不能像RNN一样利用时序的信息。
- RNN：时间序列特征导致其不能并行计算。
- DNN：隐藏层更加复杂，导致参数量过于庞大。

# 心得体会

通过《人工智能导论》第二次实验，我第一次利用python的机器学习框架实现了三种不同的神经网络框架，这对我的理论知识体系和编程能力都有很大的提升。

不过我仍旧认为机器学习实际就是**统计学**的一个应用。对于统计学知识还比较薄弱的我而言（正在修《概率论与数理统计》，而且并没有太多统计的知识），完成实验的过程仍旧囫囵吞枣的。如果能有时间接触一下公式的推导，我认为这对我的帮助将会更大。
